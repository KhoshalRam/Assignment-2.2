HDFS Cluster: 
       A Hadoop cluster is a combination of simpler computation that is used to store and compute collosal amounts of un-normalised data .
            
Elements of the HDFS cluster:
      A typical Hadoop cluster consists of 3 node types called as: 1) Master Nodes, 2)Slave Nodes and 3) Client Nodes. 
      
      Master Node: The master node takes care of the major operations in the HDFS such as storing and parallel functioning of data using name nodes.
      
      Name node: The name node will coordinate the data storage function. Simultaneously the job tracker will coordinate the parallel processing of data
      using Map Reduce.
      
      Slave Nodes:  The slave nodes constitute the major portion of the virtual machine and perform operations like storing data and computations.
                    Each slave node works on the basis of instructions that are received from the master node in regular intervals. There is regular 
                    communication from the master node. 
      Client Nodes: Client nodes are neither master or slave nodes. It sends the data into the cluster and describes how the job has to be done.
                    and then finally retrieves the results of the job processed when it is complete.
